@INPROCEEDINGS{liu2020logiqa,
    title={LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},
    author={Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},
    year={2020},
    eprint={2007.08124},
    publisher = {International Joint Conferences on Artificial Intelligence Organization (IJCAI)},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@article{Liu_Cui_Liu_Zhang_2021, title={Natural Language Inference in Context - Investigating Contextual Reasoning over Long Texts}, volume={35}, number={15}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Hanmeng and Cui, Leyang and Liu, Jian and Zhang, Yue}, year={2021}, month={May}, pages={13388-13396} }

@inproceedings{liu-etal-2021-solving,
    title = "Solving Aspect Category Sentiment Analysis as a Text Generation Task",
    author = "Liu, Jian  and
      Teng, Zhiyang  and
      Cui, Leyang  and
      Liu, Hanmeng  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    publisher = "Association for Computational Linguistics (EMNLP)",
    pages = "4406--4416",
}

@ARTICLE{10174688,
  author={Liu, Hanmeng and Liu, Jian and Cui, Leyang and Teng, Zhiyang and Duan, Nan and Zhou, Ming and Zhang, Yue},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title={LogiQA 2.0—An Improved Dataset for Logical Reasoning in Natural Language Understanding},
  year={2023},
  volume={31},
  number={},
  pages={2947-2962},
  doi={10.1109/TASLP.2023.3293046}}


@inproceedings{yang-etal-2023-glue,
    title = "{GLUE}-{X}: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective",
    author = "Yang, Linyi  and
      Zhang, Shuibai  and
      Qin, Libo  and
      Li, Yafu  and
      Wang, Yidong  and
      Liu, Hanmeng  and
      Wang, Jindong  and
      Xie, Xing  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.806",
    doi = "10.18653/v1/2023.findings-acl.806",
    pages = "12731--12750",
    abstract = "Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.",
}

@inproceedings{liu2023evaluating,
      title={Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4},
      author={Hanmeng Liu and Ruoxi Ning and Zhiyang Teng and Jian Liu and Qiji Zhou and Yue Zhang},
      year={2023},
      eprint={2304.03439},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{liu2023glore,
      title={GLoRE: Evaluating Logical Reasoning of Large Language Models},
      author={Hanmeng Liu and Zhiyang Teng and Ruoxi Ning and Chaoli Zhang and Jian Liu and Qiji Zhou and Yue Zhang},
      year={2023},
      month=Oct,
      eprint={2304.03439},
      publisher={EMNLP 2023},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{liu2023logicot,
      title={LogiCoT: Logical Chain-of-Thought Instruction-Tuning with GPT-4},
      author={Hanmeng Liu and Zhiyang Teng and Leyang Cui and Chaoli Zhang and Qiji Zhou and Yue Zhang},
      year={2023},
      month=Oct,
      eprint={2305.12147},
      publisher={EMNLP Under Review},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hanmeng-yue-2024-da,
    title = "大模型逻辑推理研究综述(Survey on Logical Reasoning of Large Pre-trained Language Models)",
    author = "Hanmeng, Liu  and
      Yue, Zhang",
    editor = "Zhao, Xin",
    booktitle = "Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)",
    month = jul,
    year = "2024",
    address = "Taiyuan, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2024.ccl-2.3/",
    pages = "48--62",
    language = "zho",
    abstract = "{\textquotedblleft}理解自然语言的逻辑结构和关系是机器理解的核心任务,也是人工智能领域的关键研究议题。随着大数据和计算能力的提升,预训练语言模型在逻辑推理方面取得了显著进展,使得大规模模型的逻辑推理能力成为研究的新焦点。本综述旨在全面梳理大模型在逻辑推理领域的研究进展,探讨其对人工智能系统智能水平评估的重要性及其在推动人工智能发展中的作用。 本文首先界定了大模型逻辑推理能力的研究范畴,系统性地讨论了逻辑推理的类型和 特点,并回顾了相关理论的发展,为研究提供了清晰的框架。接着,从任务形式和数 据基准的角度,详细介绍了逻辑推理研究的基础工作,为理解大模型的性能提供了基 准。进一步,本文深入分析了大模型在逻辑推理能力上的现状,通过不同推理类型的 案例研究,展示了大模型的能力表现。同时,本文还探讨了提升大模型逻辑推理能力 的方法,包括预训练、指令微调、解码策略和神经符号混合方法,并对这些方法进行 了比较分析。最后,本文提出了对未来研究方向的展望,旨在激发更多的学术讨论和 探索,推动逻辑推理能力研究的进一步发展。{\textquotedblright}"
}

@misc{ding2024breakchainlargelanguage,
      title={Break the Chain: Large Language Models Can be Shortcut Reasoners},
      author={Mengru Ding and Hanmeng Liu and Zhizhang Fu and Jian Song and Wenbo Xie and Yue Zhang},
      year={2024},
      eprint={2406.06580},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.06580},
}

@misc{liu2025logicalreasoninglargelanguage,
      title={Logical Reasoning in Large Language Models: A Survey},
      author={Hanmeng Liu and Zhizhang Fu and Mengru Ding and Ruoxi Ning and Chaoli Zhang and Xiaozhang Liu and Yue Zhang},
      year={2025},
      eprint={2502.09100},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.09100},
}
